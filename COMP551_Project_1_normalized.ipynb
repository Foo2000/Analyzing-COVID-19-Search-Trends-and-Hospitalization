{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8j0hsOoM03e"
   },
   "source": [
    "# Task 1: Acquire, pre-process and analyze the data\n",
    "## Acquiring both datasets:\n",
    "Dataset 1: [Search Trends](https://github.com/google-research/open-covid-19-data/blob/master/data/exports/search_trends_symptoms_dataset/README.mdhttps://)\n",
    "\n",
    "Dataset 2: [COVID hospitalization cases](https://github.com/google-research/open-covid-19-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import math\n",
    "import statistics\n",
    "from numpy import nanmedian, NaN\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the week of 08/24/2020 for the data collection\n",
    "# Load into pandas dataframes\n",
    "st_df = pd.read_csv('2020_US_weekly_symptoms_dataset.csv', low_memory=False)\n",
    "hp_df = pd.read_csv('aggregated_cc_by.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elRSVGYhZi7L"
   },
   "source": [
    "## Preprocess the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Weeks range: 2020-03-09 to 2020-09-21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search trends dataset Part I\n",
    "\n",
    "#TODO: Preprocessing, remove all symptoms that have all zero entries (clean COLUMN)\n",
    "st_df = st_df.dropna(how='all', axis=1)\n",
    "\n",
    "#Remove all rows not in the date of the week chosen (clean ROW)\n",
    "st_df = st_df[st_df['date'] >= '2020-03-04']\n",
    "\n",
    "nameList = list(st_df['sub_region_1']) #extract the region names from st_df database\n",
    "nameList = list(dict.fromkeys(nameList))  #remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospitalization dataset Part I\n",
    "\n",
    "#TODO: Preprocessing\n",
    "\n",
    "#keep the hospitalization features and delete the rest  (clean COLUMN)\n",
    "hp_df = hp_df[['open_covid_region_code','region_name','date', 'hospitalized_new']]\n",
    "\n",
    "#select the regions that match the Search trends dataset (clean ROW)\n",
    "hp_df= hp_df[hp_df.region_name.isin(nameList)]\n",
    "\n",
    "#select the regions that have the valid date range (clean ROW)\n",
    "hp_df = hp_df[(hp_df['date'] >= '2020-03-09') & (hp_df['date'] <= '2020-09-27')]\n",
    "\n",
    "hp_df.reset_index(inplace = True) \n",
    "#print(hp_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospitalization dataset Part II\n",
    "# Here we want to group dates in the same week together as one date (the weekdate)\n",
    "hp_df1 = hp_df\n",
    "weekdate = '2020-03-09'\n",
    "\n",
    "#This loop will update all the dates row by row\n",
    "for i, n in hp_df1.iterrows():\n",
    "    if (i%7 == 0):\n",
    "        weekdate = n['date']  #first date of the week\n",
    "    else:\n",
    "        hp_df1.at[i,'date'] = weekdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum up the hospitalized_vew for weekly\n",
    "# we are only using this hp_df2 to rid regions that have insignificant hospitalized data, such as 0 for total hospitalization\n",
    "def cleanRegions(df):\n",
    "    hp_df2 = df\n",
    "    f = dict.fromkeys(hp_df2.columns.difference(['region_name']), 'first')\n",
    "    f['hospitalized_new'] = sum\n",
    "    hp_df2 = hp_df2.groupby('region_name', as_index=False).agg(f)\n",
    "    hp_df2 = hp_df2[hp_df2.hospitalized_new != 0]\n",
    "    print(hp_df2.to_string())\n",
    "    tmplist = list(hp_df2['region_name']) \n",
    "    tmplist = list(dict.fromkeys(tmplist))  \n",
    "    return(tmplist)\n",
    "\n",
    "#this nameList will be a new regions list that removes region with total of 0 hospitalization value for all its dates\n",
    "nameList2 = cleanRegions(hp_df1)\n",
    "\n",
    "#filter hp_df1 based on the nameList2 (clean ROW)\n",
    "hp_df2= hp_df1[hp_df1.region_name.isin(nameList2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospitalization dataset Part III\n",
    "\n",
    "# merge 7 week rows into 1 and sum up the hospitalized_new data\n",
    "hp_df3 = hp_df2.groupby(['region_name','date'])['hospitalized_new'].apply(sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search trends dataset Part II\n",
    "\n",
    "# Drop unnecessary columns (open_covid region_code, country_region_code, country_region) (clean COLUMN)\n",
    "st_df1 = st_df.drop(st_df.columns[[0, 1, 2]], axis=1)\n",
    "\n",
    "# Filter st_df based on nameList2 (clean ROW)\n",
    "st_df1= st_df1[st_df.sub_region_1.isin(nameList2)]\n",
    "# print(st_df1)\n",
    "# print(st_df1.shape)\n",
    "\n",
    "#Filter columns so that every column have at least sp_num% of non-zero entries  (clean COLUMN)\n",
    "sp_num = 0.24  #optimized ratio without tremendous loss of dataset\n",
    "st_df2 = st_df1.dropna(thresh=sp_num*len(st_df), axis=1)\n",
    "\n",
    "\n",
    "st_df2.reset_index(inplace = True) \n",
    "st_df2 = st_df2.drop(st_df2.columns[[0]], axis=1)\n",
    "\n",
    "# print(\"after.........\" )\n",
    "#print(st_df2.head(50))\n",
    "#print(st_df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of the Search Trend Datasets:\n",
    "\n",
    "#Find each symptom's median\n",
    "#61 symptoms\n",
    "medianList = []\n",
    "for (columnName, columnData) in st_df2.iteritems(): \n",
    "    if (columnName != 'sub_region_1' and columnName != 'sub_region_1_code' and columnName != 'date'):\n",
    "        m =  nanmedian(columnData.values) \n",
    "#         print(\"my median is\", m)\n",
    "        s = columnData.size\n",
    "        i = 0\n",
    "        for i in range(s):\n",
    "            v = columnData.values[i]\n",
    "            if (math.isnan(v)):\n",
    "                columnData.values[i] = columnData.values[i]\n",
    "            else:\n",
    "#                 print(\"my val before:\", columnData.values[i])\n",
    "                columnData.values[i] = columnData.values[i]/m\n",
    "#                 print(\"my val after:\", columnData.values[i])\n",
    "            i = i+1\n",
    "\n",
    "# print(st_df2.shape)\n",
    "# print(st_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hpData = hp_df3[\"hospitalized_new\"]\n",
    "hpData = pd.Series(hpData)\n",
    "\n",
    "st_df2['hospitalized_new'] = hpData.values # Merging the data_set\n",
    "#print(st_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert merged dataset into a numpy array\n",
    "myarray = pd.DataFrame(st_df2).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Visualize and Cluster the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the evolution of the search frequency of popular symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values by 0s\n",
    "myarray[pd.isnull(myarray)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = myarray[:,0]\n",
    "time = myarray[:,2]\n",
    "features = myarray[:,3:-1].astype(float)\n",
    "label = myarray[:,-1].astype(int)\n",
    "\n",
    "nData, nFeat = features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of symptom names\n",
    "nameSymptoms = np.array([])\n",
    "for name in st_df2.columns.values[3:-1]:\n",
    "    nameSymptoms = np.append(nameSymptoms, name.lstrip('symptom:'))\n",
    "\n",
    "# print(nameSymptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most popular searches (rank by # of instances)\n",
    "mostPop = 5\n",
    "\n",
    "arrInst = np.count_nonzero(features, axis=0)\n",
    "# Get indices of most popular searches\n",
    "topSearchInd = np.sort(np.argpartition(arrInst, -mostPop)[-mostPop:])\n",
    "# print(topSearchInd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of symptom names of most popular searches\n",
    "namePopSymptoms = nameSymptoms[topSearchInd]\n",
    "# print(namePopSymptoms)\n",
    "\n",
    "# Get array of features of only the most popular searches\n",
    "popFeatures = features[:,topSearchInd]\n",
    "# print(popFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape search data 2D array into 3D [region, time, feature]\n",
    "\n",
    "uniqueRegions, ctRegions = np.unique(regions, return_counts=True)\n",
    "# print(dict(zip(uniqueRegions, ctRegions)))\n",
    "\n",
    "uniqueTime, ctTime = np.unique(time, return_counts=True)\n",
    "# print(dict(zip(uniqueTime, ctTime)))\n",
    "\n",
    "nRegions = ctTime[0]\n",
    "nTime = ctRegions[0]\n",
    "\n",
    "sdArr = popFeatures.reshape(-1, nTime, mostPop)\n",
    "# print(sdArr.shape)\n",
    "# print(sdArr[0,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap + Slider Visualization\n",
    "\n",
    "# Processing\n",
    "dataHeatmap = np.swapaxes(sdArr, 0, 2)\n",
    "hCMap = plt.cm.get_cmap('PiYG_r', 9)\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "def timeSlider(TimeSlider=0):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax.set_xticks(np.arange(nRegions))\n",
    "    ax.set_yticks(np.arange(mostPop))\n",
    "\n",
    "    ax.set_xticklabels(uniqueRegions, fontsize=15)\n",
    "    ax.set_yticklabels(namePopSymptoms, fontsize=15)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    \n",
    "    ax.set_title('Evolution of symptom search trends -- Week of ' + str(uniqueTime[TimeSlider]) + ' -- Ratio to symptom median', fontsize=20)\n",
    "    im = ax.imshow(dataHeatmap[:,TimeSlider,:], vmin=0.0, vmax=2, cmap=hCMap)\n",
    "    plt.colorbar(im).set_ticks([0, 1, 2])\n",
    "    \n",
    "    \n",
    "    fig.canvas.draw()\n",
    "        \n",
    "widgets.interact(timeSlider, TimeSlider=(0, nTime-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(mostPop):\n",
    "    fig = plt.figure(facecolor='white', figsize=(20,10))\n",
    "    plt.axhline(1, color='black', lw=2)\n",
    "    plt.xticks(np.arange(nTime), uniqueTime, rotation=45, ha='right', rotation_mode='anchor')\n",
    "    plt.yticks(np.arange(0, 5, step=0.25))\n",
    "    plt.title('Evolution of ' + namePopSymptoms[i] + ' -- Average ratio to symptom median across all regions', fontsize=20)\n",
    "    plt.xlabel('Time', fontsize=15)\n",
    "    plt.ylabel('Normalized search frequency', fontsize=15)\n",
    "    \n",
    "    avgSDArr = np.average(sdArr[:,:,i], axis=0)\n",
    "    barColors = []\n",
    "    for avgNum in avgSDArr:\n",
    "        if avgNum > 1:\n",
    "            barColors.append(hCMap(7))\n",
    "        else:\n",
    "            barColors.append(hCMap(1))\n",
    "    \n",
    "    plt.bar(x=np.arange(nTime), height=avgSDArr,width=0.9, bottom=0, color=barColors)\n",
    "    plt.grid(b=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA to reduce data dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the features\n",
    "pca = PCA()\n",
    "pca.fit(features)\n",
    "pcaEVR = pca.explained_variance_ratio_\n",
    "totalPC = len(pcaEVR)\n",
    "cumulativeVar = 100*np.cumsum(pcaEVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshCVar = 95\n",
    "\n",
    "nPC = np.where(cumulativeVar > threshCVar)[0][0]\n",
    "cVar = cumulativeVar[nPC]\n",
    "nPC += 1\n",
    "\n",
    "# print(cVar, nPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative variance vs # of principal components to choose #PCs\n",
    "fig = plt.figure()\n",
    "plt.plot(np.linspace(1, totalPC, totalPC), threshCVar*np.ones((totalPC,)), 'c--')\n",
    "plt.plot(np.linspace(1, totalPC, totalPC), cumulativeVar, 'x')\n",
    "plt.plot(nPC, cVar, 'o')\n",
    "\n",
    "plt.text(nPC-3, cVar-4, 'Optimal #PCs: ' + str(nPC))\n",
    "plt.text(totalPC, threshCVar, 'Cumulative Variance Threshold:' + str(threshCVar) + '%', horizontalalignment='right')\n",
    "\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"% Variance Explained\")\n",
    "plt.title(\"Cumulative Variance Explained vs Number of Principal Components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have chosen the #PCs -> now reduce the search trends dataset to this dimensionality\n",
    "pcaRed = PCA(n_components=nPC)\n",
    "pcaRed.fit(features)\n",
    "reducedFeat = pcaRed.transform(features)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(reducedFeat[:,0], reducedFeat[:,1])\n",
    "plt.xlabel(\"PC #1\")\n",
    "plt.ylabel(\"PC #2\")\n",
    "plt.title(\"Search Trends Dataset Reduced to \" + str(nPC) + \"D, Visualizing in 2D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k-means clustering to evaluate groups in search trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the elbow method to determine K for the reduced data\n",
    "nClusterRange = range(1, 20)\n",
    "inertiaRed = []\n",
    "inertiaOri = []\n",
    "\n",
    "for clst in nClusterRange:\n",
    "    kmeansRed = KMeans(n_clusters=clst, random_state=0)\n",
    "    kmeansRed.fit(reducedFeat)\n",
    "    inertiaRed.append(kmeansRed.inertia_)\n",
    "    \n",
    "    kmeansOri = KMeans(n_clusters=clst, random_state=0)\n",
    "    kmeansOri.fit(features)\n",
    "    inertiaOri.append(kmeansOri.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the inertia\n",
    "plt.figure(figsize=(21,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nClusterRange, inertiaRed, 'x', markersize=15)\n",
    "plt.plot(6, inertiaRed[6-1], 'X', markersize=20)\n",
    "plt.text(6.5, inertiaRed[6-1] + 300, 'Optimal: K = 6', fontsize=16)\n",
    "plt.xticks(nClusterRange)\n",
    "plt.xlabel('K', fontsize=16)\n",
    "plt.ylabel('Inertia', fontsize=16)\n",
    "plt.title('K-Means -- Inertia of reduced data versus the number of clusters (K)', fontsize=16)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(nClusterRange, inertiaOri, 'x', markersize=15)\n",
    "plt.plot(6, inertiaOri[6-1], 'X', markersize=20)\n",
    "plt.text(6.5, inertiaOri[6-1] + 300, 'Optimal: K = 6', fontsize=16)\n",
    "plt.xticks(nClusterRange)\n",
    "plt.xlabel('K', fontsize=16)\n",
    "plt.ylabel('Inertia', fontsize=16)\n",
    "plt.title('K-Means -- Inertia of unreduced (original) data versus the number of clusters (K)', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the optimal K (nClusters):\n",
    "nClusters = 6\n",
    "\n",
    "# Clustering on PCA-reduced data\n",
    "kmeansRed = KMeans(n_clusters=nClusters, random_state=0)\n",
    "kmeansRed.fit(reducedFeat)\n",
    "y_PredRed = kmeansRed.predict(reducedFeat)\n",
    "\n",
    "# Clustering on original (non-reduced) data\n",
    "kmeansOri = KMeans(n_clusters=nClusters, random_state=0)\n",
    "kmeansOri.fit(features)\n",
    "y_PredOri = kmeansOri.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot clustering results for both reduced and unreduced data\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(reducedFeat[:,0], reducedFeat[:,1], c=y_PredRed, cmap=plt.cm.get_cmap('viridis', nClusters))\n",
    "plt.colorbar(ticks=range(nClusters))\n",
    "plt.xlabel(\"PC #1\", fontsize=16)\n",
    "plt.ylabel(\"PC #2\", fontsize=16)\n",
    "plt.title(\"Cluster Labels for \" + str(nPC) + \"D K-Means with \" + str(nClusters) + \" Clusters\", fontsize=16)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(reducedFeat[:,0], reducedFeat[:,1], c=y_PredOri, cmap=plt.cm.get_cmap('viridis', nClusters))\n",
    "plt.colorbar(ticks=range(nClusters))\n",
    "plt.xlabel(\"PC #1\", fontsize=16)\n",
    "plt.ylabel(\"PC #2\", fontsize=16)\n",
    "plt.title(\"Cluster Labels for \" + str(totalPC) + \"D K-Means (Unreduced Data) with \" + str(nClusters) + \" Clusters\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from  sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace NaN values to be 0's\n",
    "myarray[pd.isnull(myarray)] = 0\n",
    "print(myarray.shape)\n",
    "\n",
    "#data_r is sorted based on regions\n",
    "data_r = myarray\n",
    "#data_t is sorted based on time\n",
    "data_t = np.array(sorted(myarray,key = lambda x: x[2]))\n",
    "\n",
    "#split the region-based data to be regions, features, and label\n",
    "regions_r = data_r[:,0]\n",
    "features_r = data_r[:,3:-1].astype(float)\n",
    "label_r = data_r[:,-1].astype(int)\n",
    "#split the time-based data to be time, features, and label\n",
    "time_t = data_t[:,2]\n",
    "features_t = data_t[:,3:-1].astype(float)\n",
    "label_t = data_t[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region-based cross-validation (5-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the region-based data into 5 folds based on regions\n",
    "from sklearn.model_selection import GroupKFold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "#specify the group indices by regions\n",
    "groups = regions_r\n",
    "for i in range(groups.shape[0]):\n",
    "    if groups[i] == 'Hawaii' or groups[i] ==  'Idaho':\n",
    "        groups[i] = 1\n",
    "    elif groups[i] == 'Maine' or groups[i] ==  'Montana':\n",
    "        groups[i] = 2\n",
    "    elif groups[i] ==  'North Dakota' or groups[i] ==  'Nebraska':\n",
    "        groups[i] = 3\n",
    "    elif groups[i] ==  'New Hampshire' or groups[i] ==  'New Mexico':\n",
    "        groups[i] = 4\n",
    "    else:\n",
    "        groups[i] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (region-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot cross-validated MSE error of KNN with respect to different values of k\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "k_range_r = range(1, 18)\n",
    "k_scores_r = []\n",
    "for k in k_range_r:\n",
    "    knn_r = neighbors.KNeighborsRegressor(n_neighbors=k)\n",
    "    loss = abs(cross_val_score(knn_r, features_r,label_r, cv=gkf.split(features_r,label_r, groups), scoring='neg_mean_squared_error'))\n",
    "    k_scores_r.append(loss.mean())\n",
    "plt.plot(k_range_r, k_scores_r)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=11 can represent the performance of KNN\n",
    "knn_r = neighbors.KNeighborsRegressor(n_neighbors=11)\n",
    "\n",
    "start = time.time()\n",
    "loss = abs(cross_val_score(knn_r, features_r,label_r, cv=gkf.split(features_r,label_r, groups), scoring='neg_mean_squared_error'))\n",
    "stop = time.time()\n",
    "\n",
    "knn_r_error = loss.mean()\n",
    "train_time = stop - start\n",
    "print(\"The error for KNN (region-based validation) is \" + str(knn_r_error) + \".\\n\" +\n",
    "      \"The training time for KNN (region-based validation) is \" + str(train_time) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree (region-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot cross-validated MSE error of decision tree with respect to different values of max_depth\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "max_depth_range_r = range(1, 30)\n",
    "max_depth_scores_r = []\n",
    "for max_depth in max_depth_range_r:\n",
    "    dt_r = tree.DecisionTreeRegressor(max_depth=max_depth, random_state=1)\n",
    "    loss = abs(cross_val_score(dt_r, features_r,label_r, cv=gkf.split(features_r,label_r, groups), scoring='neg_mean_squared_error'))\n",
    "    max_depth_scores_r.append(loss.mean())\n",
    "plt.plot(max_depth_range_r, max_depth_scores_r)\n",
    "plt.xlabel('Value of max_depth for decision tree')\n",
    "plt.ylabel('Cross-Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_depth = 2 can represent the performance of decision tree\n",
    "dt_r = tree.DecisionTreeRegressor(max_depth = 2, random_state=1)\n",
    "\n",
    "start = time.time()\n",
    "loss = abs(cross_val_score(dt_r, features_r, label_r, cv = gkf.split(features_r,label_r, groups), scoring='neg_mean_squared_error'))\n",
    "stop = time.time()\n",
    "\n",
    "dt_r_error = loss.mean()\n",
    "train_time = stop - start\n",
    "print(\"The error for decision tree (region-based validation) is \" + str(dt_r_error) + \".\\n\" +\n",
    "      \"The training time for decision tree (region-based validation) is \" + str(train_time) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-based validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test sets based on the timepoint '2020-08-10'\n",
    "#data before '2020-08-10' are in train set and the rest are in test set\n",
    "#time_t[242] is the first '2020-08-10' which is the splitting point\n",
    "features_t_train = features_t[0:241]\n",
    "label_t_train = label_t[0:241]\n",
    "features_t_test = features_t[241:320]\n",
    "label_t_test = label_t[241:320]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (time-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot validated MSE error of KNN with respect to different values of k\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "k_range_t = range(1, 18)\n",
    "k_scores_t= []\n",
    "for k in k_range_t:\n",
    "    knn_t = neighbors.KNeighborsRegressor(n_neighbors = k)\n",
    "    knn_t.fit(features_t_train, label_t_train)\n",
    "    label_t_pred_knn = knn_t.predict(features_t_test)\n",
    "    loss = mean_squared_error(label_t_test, label_t_pred_knn)\n",
    "    k_scores_t.append(loss)\n",
    "plt.plot(k_range_t, k_scores_t)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=15 can represent the performance of KNN\n",
    "knn_t = neighbors.KNeighborsRegressor(n_neighbors=15)\n",
    "\n",
    "start = time.time()\n",
    "knn_t.fit(features_t_train, label_t_train)\n",
    "label_t_pred_knn = knn_t.predict(features_t_test)\n",
    "knn_t_error = mean_squared_error(label_t_test, label_t_pred_knn)\n",
    "stop = time.time()\n",
    "\n",
    "train_time = stop - start\n",
    "print(\"The error for KNN (time-based validation) is \" + str(knn_t_error) + \".\\n\" +\n",
    "      \"The training time for KNN (time-based validation) is \" + str(train_time) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree (time-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot validated MSE error of decision tree with respect to different values of max_depth\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "max_depth_range_t = range(1, 30)\n",
    "max_depth_scores_t = []\n",
    "for max_depth in max_depth_range_t:\n",
    "    dt_t = tree.DecisionTreeRegressor(max_depth=max_depth, random_state=1)\n",
    "    dt_t.fit(features_t_train, label_t_train)\n",
    "    label_t_pred_dt = dt_t.predict(features_t_test)\n",
    "    loss = mean_squared_error(label_t_test, label_t_pred_dt)\n",
    "    max_depth_scores_t.append(loss)\n",
    "plt.plot(max_depth_range_t, max_depth_scores_t)\n",
    "plt.xlabel('Value of max_depth for decision tree')\n",
    "plt.ylabel('Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_depth = 1 can represent the performance of decision tree\n",
    "dt_t = tree.DecisionTreeRegressor(max_depth = 1, random_state=1)\n",
    "\n",
    "start = time.time()\n",
    "dt_t.fit(features_t_train, label_t_train)\n",
    "label_t_pred_dt = dt_t.predict(features_t_test)\n",
    "dt_t_error = mean_squared_error(label_t_test, label_t_pred_dt)\n",
    "stop = time.time()\n",
    "\n",
    "train_time = stop - start\n",
    "print(\"The error for decison tree (time-based validation) is \" + str(dt_t_error) + \".\\n\" +\n",
    "      \"The training time for decison tree (time-based validation) is \" + str(train_time) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional part: learn seperate model for each region with time-based validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regional KNN models (time-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot average validated KNN MSE error of all regions with respect to different values of k\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "k_range_regional = range(1, 18)\n",
    "k_scores_regional= []\n",
    "\n",
    "for k in k_range_regional:\n",
    "    regional_MSE_knn = []\n",
    "    for i in range(0,groups.shape[0],29):\n",
    "        #each iteration deal with data from one city\n",
    "        regional_features = features_r[i:i+29]\n",
    "        regional_label = label_r[i:i+29]\n",
    "        #split the data into train and test sets based on the timepoint 2020-08-10\n",
    "        #data before 2020-08-10 are in train set and the rest are in test set\n",
    "        #for data of each region, the 24th. datum is of '2020-08-10' which is the splitting point\n",
    "        regional_features_train = regional_features[0:23]\n",
    "        regional_label_train = regional_label[0:23]\n",
    "        regional_features_test = regional_features[23:29]\n",
    "        regional_label_test = regional_label[23:29]\n",
    "        #calculate the KNN MSE of the city of this iteration\n",
    "        knn = neighbors.KNeighborsRegressor(n_neighbors = k)\n",
    "        knn.fit(regional_features_train, regional_label_train)\n",
    "        regional_label_pred_knn = knn.predict(regional_features_test)\n",
    "        loss_knn = mean_squared_error(regional_label_test, regional_label_pred_knn)\n",
    "        regional_MSE_knn.append(loss_knn)\n",
    "    #append the average validated MSE of the current k\n",
    "    regional_MSE_knn = np.array(regional_MSE_knn)\n",
    "    k_scores_regional.append(regional_MSE_knn.mean())\n",
    "\n",
    "plt.plot(k_range_regional, k_scores_regional)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Average Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=15 can represent the performance of KNN\n",
    "regional_MSE_knn = []\n",
    "\n",
    "start = time.time()\n",
    "for i in range(0,groups.shape[0],29):\n",
    "    #each iteration deal with data from one city\n",
    "    regional_features = features_r[i:i+29]\n",
    "    regional_label = label_r[i:i+29]\n",
    "    #split the data into train and test sets based on the timepoint 2020-08-10\n",
    "    #data before 2020-08-10 are in train set and the rest are in test set\n",
    "    #for data of each region, the 24th. datum is of '2020-08-10' which is the splitting point\n",
    "    regional_features_train = regional_features[0:23]\n",
    "    regional_label_train = regional_label[0:23]\n",
    "    regional_features_test = regional_features[23:29]\n",
    "    regional_label_test = regional_label[23:29]\n",
    "    #calculate the KNN MSE of the city of this iteration\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors = 15)\n",
    "    knn.fit(regional_features_train, regional_label_train)\n",
    "    regional_label_pred_knn = knn.predict(regional_features_test)\n",
    "    loss_knn = mean_squared_error(regional_label_test, regional_label_pred_knn)\n",
    "    regional_MSE_knn.append(loss_knn)\n",
    "stop = time.time()\n",
    "\n",
    "train_time = stop - start\n",
    "regional_MSE_knn = np.array(regional_MSE_knn)\n",
    "knn_regional_error = regional_MSE_knn.mean()\n",
    "print(\"The error for regional KNN models (time-based validation) is \" + str(knn_regional_error) + \".\\n\" +\n",
    "      \"The training time for regional KNN models (time-based validation) is \" + str(train_time) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regional decision tree models (time-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot average validated decision tree MSE error of all regions with respect to different values of max_depth\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "max_depth_range_regional = range(1, 30)\n",
    "dt_scores_regional= []\n",
    "\n",
    "for max_depth in max_depth_range_regional:\n",
    "    regional_MSE_dt = []\n",
    "    for i in range(0,groups.shape[0],29):\n",
    "        #each iteration deal with data from one city\n",
    "        regional_features = features_r[i:i+29]\n",
    "        regional_label = label_r[i:i+29]\n",
    "        #split the data into train and test sets based on the timepoint 2020-08-10\n",
    "        #data before 2020-08-10 are in train set and the rest are in test set\n",
    "        #for data of each region, the 24th. datum is of '2020-08-10' which is the splitting point\n",
    "        regional_features_train = regional_features[0:23]\n",
    "        regional_label_train = regional_label[0:23]\n",
    "        regional_features_test = regional_features[23:29]\n",
    "        regional_label_test = regional_label[23:29]\n",
    "        #calculate the KNN MSE of the city of this iteration\n",
    "        dt = tree.DecisionTreeRegressor(max_depth = max_depth, random_state=1)\n",
    "        dt.fit(regional_features_train, regional_label_train)\n",
    "        regional_label_pred_dt = dt.predict(regional_features_test)\n",
    "        loss_dt = mean_squared_error(regional_label_test, regional_label_pred_dt)\n",
    "        regional_MSE_dt.append(loss_dt)\n",
    "    #append the average validated MSE of the current k\n",
    "    regional_MSE_dt = np.array(regional_MSE_dt)\n",
    "    dt_scores_regional.append(regional_MSE_dt.mean())\n",
    "\n",
    "plt.plot(max_depth_range_regional, dt_scores_regional)\n",
    "plt.xlabel('Value of max_depth for decision tree')\n",
    "plt.ylabel('Average Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_depth = 2 can represent the performance of decision tree\n",
    "regional_MSE_dt = []\n",
    "\n",
    "start = time.time()\n",
    "for i in range(0,groups.shape[0],29):\n",
    "    #each iteration deal with data from one city\n",
    "    regional_features = features_r[i:i+29]\n",
    "    regional_label = label_r[i:i+29]\n",
    "    #split the data into train and test sets based on the timepoint 2020-08-10\n",
    "    #data before 2020-08-10 are in train set and the rest are in test set\n",
    "    #for data of each region, the 24th. datum is of '2020-08-10' which is the splitting point\n",
    "    regional_features_train = regional_features[0:23]\n",
    "    regional_label_train = regional_label[0:23]\n",
    "    regional_features_test = regional_features[23:29]\n",
    "    regional_label_test = regional_label[23:29]\n",
    "    #calculate the decision tree MSE of the city of this iteration\n",
    "    dt = tree.DecisionTreeRegressor(max_depth = 2, random_state=1)\n",
    "    dt.fit(regional_features_train, regional_label_train)\n",
    "    regional_label_pred_dt = dt.predict(regional_features_test)\n",
    "    loss_dt = mean_squared_error(regional_label_test, regional_label_pred_dt)\n",
    "    regional_MSE_dt.append(loss_dt)\n",
    "stop = time.time()\n",
    "\n",
    "train_time = stop - start\n",
    "regional_MSE_dt = np.array(regional_MSE_dt)\n",
    "dt_regional_error = regional_MSE_dt.mean()\n",
    "print(\"The error for regional decision tree models (time-based validation) is \" + str(dt_regional_error) + \".\\n\" +\n",
    "      \"The training time for regional decision tree models (time-based validation) is \" + str(train_time) + \".\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN0fg+0w8Z9nRsJs3Ucih0p",
   "include_colab_link": true,
   "name": "COMP551_Project_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
